{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb58b091-316e-42e4-b225-faf0d08014e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\TravelMate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "import pymysql\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5ae830b-66a1-483b-bb4c-a1eafad35d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to the MySQL database\n",
    "connection = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='',\n",
    "    database='similaritydb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ea168b-9f5f-4f73-9bde-b3417ed98963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8afb509d-9dd6-4002-8cbd-59986e207f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fetch data from the database\n",
    "query = \"SELECT * FROM similaritydataset\"\n",
    "cursor.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4762aa11-02cc-4bdc-8395-c824988b8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch all rows from the executed query\n",
    "rows = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "917210de-08a8-443a-b09f-bc374783f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get column names from the database\n",
    "columns = [col[0] for col in cursor.description]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c14dd5f3-314d-4b4f-a575-627a7062c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a pandas DataFrame from the fetched data\n",
    "data = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da673350-bd99-450e-a0fa-e799255a376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean text function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39646c43-4db2-43ab-984b-264d485d63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the preprocessing to the required columns\n",
    "data[\"source_text\"] = data[\"source_text\"].apply(preprocess_text)\n",
    "data[\"plagiarized_text\"] = data[\"plagiarized_text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1702f02b-5a07-4d08-9765-a42d501d4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index_id                                        source_text  \\\n",
      "0         1  online cash loan transaction decision support ...   \n",
      "1         2  record management system human resources offic...   \n",
      "2         3  solar powered feeder pet fish management syste...   \n",
      "3         4  signstospeech mobile app translating sign lang...   \n",
      "4         5  webbased complaint managementsysytemforunlawfu...   \n",
      "\n",
      "                                    plagiarized_text  label  \n",
      "0  online cash loan transaction decision support ...      1  \n",
      "1  record management system human resources offic...      1  \n",
      "2  solar powered feeder pet fish management syste...      1  \n",
      "3  signstospeech mobile app translating sign lang...      1  \n",
      "4  webbased complaint managementsysytemforunlawfu...      1  \n",
      "label\n",
      "0    239\n",
      "1    180\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Check value counts for labels\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cadeacd-2f43-451e-8e4e-7eb237b3ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(data[\"source_text\"] + \" \" + data[\"plagiarized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb15054d-ba85-4282-96a7-7d5e8e2888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0343735-3944-4105-a5ce-2084d4298011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29bb312d-63ea-4555-945b-29e57133a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7738095238095238\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        47\n",
      "           1       1.00      0.49      0.65        37\n",
      "\n",
      "    accuracy                           0.77        84\n",
      "   macro avg       0.86      0.74      0.74        84\n",
      "weighted avg       0.84      0.77      0.75        84\n",
      "\n",
      "Confusion Matrix\n",
      "[[47  0]\n",
      " [19 18]]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2607252-1279-4c02-a955-c19cd4d78f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8214285714285714\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        47\n",
      "           1       1.00      0.59      0.75        37\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.88      0.80      0.80        84\n",
      "weighted avg       0.86      0.82      0.81        84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "#random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Instantiate the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2519418f-fb1a-455a-a3fc-d66b3c008d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92        47\n",
      "           1       0.97      0.81      0.88        37\n",
      "\n",
      "    accuracy                           0.90        84\n",
      "   macro avg       0.92      0.89      0.90        84\n",
      "weighted avg       0.91      0.90      0.90        84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [ 7 30]]\n"
     ]
    }
   ],
   "source": [
    "#naiv bays model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Instantiate the model\n",
    "model = MultinomialNB()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21f11007-5b46-4de7-801d-f7a4a67ac294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        47\n",
      "           1       1.00      0.78      0.88        37\n",
      "\n",
      "    accuracy                           0.90        84\n",
      "   macro avg       0.93      0.89      0.90        84\n",
      "weighted avg       0.92      0.90      0.90        84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [ 8 29]]\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the model\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Generate classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71b7913f-fe16-45ee-aea0-e296b2343ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9047619047619048\n",
      "Best Parameters: {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92        48\n",
      "           1       0.97      0.81      0.88        36\n",
      "\n",
      "    accuracy                           0.90        84\n",
      "   macro avg       0.92      0.89      0.90        84\n",
      "weighted avg       0.91      0.90      0.90        84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  1]\n",
      " [ 7 29]]\n",
      "Cross-validated Accuracy: 0.8926563396442914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the data into variables\n",
    "source_text = data[\"source_text\"]\n",
    "plagiarized_text = data[\"plagiarized_text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "# Step 1: TF-IDF Vectorization with optimal settings\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(source_text + \" \" + plagiarized_text)\n",
    "\n",
    "# Step 2: Train-Test Split (stratified to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Hyperparameter Tuning with GridSearchCV (Linear + RBF kernels)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear'],  # Stick to linear for sparse data\n",
    "    'class_weight': ['balanced', None]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for better cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Get the best model and make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 6: Print Results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Step 7: Cross-Validation for Stability Check\n",
    "cross_val_scores = cross_val_score(best_model, X, y, cv=cv, scoring='accuracy')\n",
    "print(\"Cross-validated Accuracy:\", cross_val_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89208250-eda8-4190-b7c7-7a6d461b4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save SVM and Vectorizor\n",
    "import pickle\n",
    "\n",
    "pickle.dump(model,open(\"model.pkl\",'wb'))\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3640303-852d-458e-8d0a-76393a1788af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model and Vectorizer\n",
    "model = pickle.load(open('model.pkl','rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "744ed806-1921-417e-83e5-2b2397f74d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detection System\n",
    "def detect(input_text):\n",
    "    vectorized_text = tfidf_vectorizer.transform([input_text])\n",
    "    result = model.predict(vectorized_text)\n",
    "    return \"Plagiarim Detected\" if result[0] == 1 else \"No Plagiarism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d29d11fe-0847-4fb0-9536-d32e7c0bf494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plagiarism Detected'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example ( it is a plagarized text)\n",
    "input_text = 'rice staple food nearly half worlds population continued increased production meet enhanced demand due everincreasing population faces many challenges per capita availability land water increasing fast rate worldwide mass ruraltourban movement youth search better livelihood reducing availability farm labor rice going suffer due changes high water labor requirements development highyielding varietieshybrids rice concomitant use high levels fertilizer specially nitrogen two major drivers increased rice production last four decades overuse fertilizer nitrogen created environmental problems greenhouse warming depletion ozone layer eutrophication surface groundwaters global concern nitrogen application rates rice therefore reduced may affect production nitrogen use efficiency rice lowest among cereals therefore urgent need developing nitrogen useefficient varieties rice production technologies demanding lesser water labor nitrogen pesticides achieving going herculean task agricultural scientists'\n",
    "detect(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb881c7c-0a1d-47e7-a278-300af0d1a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Plagiarism\n"
     ]
    }
   ],
   "source": [
    "# Ensure correct saving of the model and vectorizer\n",
    "import pickle\n",
    "\n",
    "# Save the best model and vectorizer\n",
    "with open(\"model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# Load the saved model and vectorizer correctly\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Detection system function\n",
    "def detect(input_text):\n",
    "    # Ensure input text is vectorized with the loaded vectorizer\n",
    "    vectorized_text = tfidf_vectorizer.transform([input_text])\n",
    "    result = model.predict(vectorized_text)\n",
    "    return \"Plagiarism Detected\" if result[0] == 1 else \"No Plagiarism\"\n",
    "\n",
    "# Example input (plagiarized text)\n",
    "input_text = (\n",
    "    'rice staple food nearly half worlds population continued increased '\n",
    "    'production meet enhanced demand due everincreasing population faces '\n",
    "    'many challenges per capita availability land water increasing fast rate...'\n",
    ")\n",
    "\n",
    "# Run the detection function\n",
    "print(detect(input_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8cbce-14e1-4926-9e8e-33c2fcefc28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
